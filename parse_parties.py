import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from google.colab import files
import os

def find_html_file():
    """–ù–∞—Ö–æ–¥–∏—Ç HTML —Ñ–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    for file in os.listdir('.'):
        if file.endswith('.html'):
            print(f"üìÅ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file}")
            return file
    return None

def parse_parties():
    """–ü–∞—Ä—Å–∏—Ç –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä—Ç–∏–∏ –∏–∑ HTML —Ñ–∞–π–ª–∞"""
    
    # –ò—â–µ–º HTML —Ñ–∞–π–ª
    html_file = find_html_file()
    
    if not html_file:
        print("HTML —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–ó–∞–≥—Ä—É–∑–∏—Ç–µ HTML —Ñ–∞–π–ª:")
        uploaded = files.upload()
        for filename in uploaded.keys():
            if filename.endswith('.html'):
                html_file = filename
                break
    
    if not html_file:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ HTML —Ñ–∞–π–ª")
        return []
    
    try:
        # –ß–∏—Ç–∞–µ–º HTML —Ñ–∞–π–ª
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        print(f"–§–∞–π–ª '{html_file}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return []
    
    soup = BeautifulSoup(html_content, 'html.parser')
    parties = []
    
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    
    # –°–ü–û–°–û–ë 1: –ò—â–µ–º –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
    tables = soup.find_all('table')
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
    
    for table in tables:
        rows = table.find_all('tr')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 2:
                name = cells[0].get_text(strip=True)
                if name and len(name) > 10:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
                    doc_url = None
                    for cell in cells:
                        links = cell.find_all('a', href=True)
                        for link in links:
                            href = link.get('href')
                            if href and ('.pdf' in href.lower() or '/documents/' in href):
                                doc_url = normalize_url(href)
                                break
                        if doc_url:
                            break
                    
                    parties.append({
                        'name': clean_party_name(name),
                        'doc_url': doc_url
                    })
    
    # –°–ü–û–°–û–ë 2: –ò—â–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if not parties:
        print("–ò—â—É —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
        doc_links = soup.find_all('a', href=lambda x: x and '/documents/' in x)
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(doc_links)}")
        
        for link in doc_links:
            name = link.get_text(strip=True)
            if name and len(name) > 10:
                doc_url = normalize_url(link.get('href'))
                parties.append({
                    'name': clean_party_name(name),
                    'doc_url': doc_url
                })
    
    # –°–ü–û–°–û–ë 3: –ò—â–µ–º –ø–æ —Å–ø–∏—Å–∫–∞–º
    if not parties:
        print("–ò—â—É –≤ —Å–ø–∏—Å–∫–∞—Ö...")
        lists = soup.find_all(['ul', 'ol'])
        for list_elem in lists:
            items = list_elem.find_all('li')
            for item in items:
                name = item.get_text(strip=True)
                if name and len(name) > 10 and is_party_name(name):
                    doc_url = extract_document_url(item)
                    parties.append({
                        'name': clean_party_name(name),
                        'doc_url': doc_url
                    })
    
    # –°–ü–û–°–û–ë 4: –ò—â–µ–º –ø–æ –¥–∏–≤–∞–º —Å –∫–ª–∞—Å—Å–∞–º–∏
    if not parties:
        print("–ò—â—É –ø–æ –∫–ª–∞—Å—Å–∞–º...")
        divs = soup.find_all('div', class_=True)
        for div in divs:
            classes = ' '.join(div.get('class', []))
            if any(word in classes.lower() for word in ['party', 'item', 'document']):
                name = div.get_text(strip=True)
                if name and len(name) > 10 and is_party_name(name):
                    doc_url = extract_document_url(div)
                    parties.append({
                        'name': clean_party_name(name),
                        'doc_url': doc_url
                    })
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_parties = []
    seen_names = set()
    
    for party in parties:
        if party['name'] not in seen_names:
            seen_names.add(party['name'])
            unique_parties.append(party)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    unique_parties.sort(key=lambda x: x['name'])
    
    return unique_parties

def clean_party_name(name):
    """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä—Ç–∏–∏"""
    if not name:
        return name
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    prefixes = [
        '–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –ø–∞—Ä—Ç–∏—è',
        '–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –ü–∞—Ä—Ç–∏—è', 
        '–ü–∞—Ä—Ç–∏—è',
        '–ù–∞–∑–≤–∞–Ω–∏–µ:',
        '–°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'
    ]
    
    for prefix in prefixes:
        if name.startswith(prefix):
            name = name.replace(prefix, '').strip()
    
    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    name = name.replace('"', '').strip()
    name = ' '.join(name.split())
    
    return name

def extract_document_url(element):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    if not element:
        return None
    
    # –ò—â–µ–º PDF —Å—Å—ã–ª–∫–∏
    pdf_links = element.find_all('a', href=lambda x: x and '.pdf' in x.lower())
    if pdf_links:
        return normalize_url(pdf_links[0].get('href'))
    
    # –ò—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏
    links = element.find_all('a', href=True)
    for link in links:
        href = link.get('href')
        if href and ('/documents/' in href or 'download' in href.lower()):
            return normalize_url(href)
    
    return None

def normalize_url(url):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL"""
    if not url:
        return None
    
    # –î–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
    if url.startswith('/'):
        url = urljoin('https://minjust.gov.ru', url)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
    if url.startswith('http://'):
        url = url.replace('http://', 'https://')
    
    return url

def is_party_name(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ø–∞—Ä—Ç–∏–∏"""
    if not text or len(text) < 10 or len(text) > 200:
        return False
    
    keywords = ['–ø–∞—Ä—Ç–∏—è', '—Ä–æ—Å—Å–∏', '–¥–µ–º–æ–∫—Ä–∞—Ç', '—Å–æ—é–∑', '–¥–≤–∏–∂–µ–Ω–∏–µ', '–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ']
    return any(keyword in text.lower() for keyword in keywords)

def main():
    print("=" * 60)
    print("–ü–ê–†–°–ò–ù–ì –ü–û–õ–ò–¢–ò–ß–ï–°–ö–ò–• –ü–ê–†–¢–ò–ô")
    print("=" * 60)
    
    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
    parties = parse_parties()
    
    if parties:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open('parties.json', 'w', encoding='utf-8') as f:
            json.dump(parties, f, ensure_ascii=False, indent=2)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä—Ç–∏–π: {len(parties)}")
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ parties.json")
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("–°–ü–ò–°–û–ö –ü–ê–†–¢–ò–ô:")
        for i, party in enumerate(parties, 1):
            doc_status = party['doc_url'] if party['doc_url'] else "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
            print(f"{i:2d}. {party['name']}")
            if party['doc_url']:
                print(f"     {party['doc_url']}")
            print()
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print("–°–∫–∞—á–∏–≤–∞—é —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º...")
        files.download('parties.json')
        
    else:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–∞—Ä—Ç–∏–∏")
     

if __name__ == "__main__":
    main()
